{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Drive\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "path_to_root_dir = \"/content/drive/MyDrive/deep learning folder/ensembleLSTM-CNN hybrid model/seq2seq\" # You are advised to created a folder in your drive with name `deep learning folder` for everything to work seamlessly or You can change this to your desired folder path\n",
    "\n",
    "if os.path.exists(path_to_root_dir) == False:\n",
    "  os.mkdir(path_to_root_dir)\n",
    "\n",
    "# Change directory to Where you want to save results\n",
    "os.chdir(path_to_root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import spacy\n",
    "from torchtext.data.metrics import bleu_score\n",
    "import sys\n",
    "\n",
    "\n",
    "def translate_sentence(model, sentence, german, english, device, max_length=50):\n",
    "    # Load german tokenizer\n",
    "    spacy_ger = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n",
    "    if type(sentence) == str:\n",
    "        tokens = [token.text.lower() for token in spacy_ger(sentence)]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "\n",
    "    # Add <SOS> and <EOS> in beginning and end respectively\n",
    "    tokens.insert(0, german.init_token)\n",
    "    tokens.append(german.eos_token)\n",
    "\n",
    "    # Go through each german token and convert to an index\n",
    "    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n",
    "\n",
    "    # Convert to Tensor\n",
    "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
    "\n",
    "    outputs = [english.vocab.stoi[\"<sos>\"]]\n",
    "    for i in range(max_length):\n",
    "        trg_tensor = torch.LongTensor(outputs).unsqueeze(1).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(sentence_tensor, trg_tensor)\n",
    "\n",
    "        best_guess = output.argmax(2)[-1, :].item()\n",
    "        outputs.append(best_guess)\n",
    "\n",
    "        if best_guess == english.vocab.stoi[\"<eos>\"]:\n",
    "            break\n",
    "\n",
    "    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n",
    "    # remove start token\n",
    "    return translated_sentence[1:]\n",
    "\n",
    "\n",
    "def bleu(data, model, german, english, device):\n",
    "    targets = []\n",
    "    outputs = []\n",
    "\n",
    "    for example in data:\n",
    "        src = vars(example)[\"src\"]\n",
    "        trg = vars(example)[\"trg\"]\n",
    "\n",
    "        prediction = translate_sentence(model, src, german, english, device)\n",
    "        prediction = prediction[:-1]  # remove <eos> token\n",
    "\n",
    "        targets.append([trg])\n",
    "        outputs.append(prediction)\n",
    "\n",
    "    return bleu_score(outputs, targets)\n",
    "\n",
    "\n",
    "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint, model, optimizer):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchtext==0.6.0\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter # to print to tensorboard\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "import numpy as np\n",
    "import spacy\n",
    "import random\n",
    "\n",
    "spacy.cli.download(\"de_core_news_sm\")\n",
    "spacy_ger = spacy.load('de_core_news_sm')\n",
    "\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "spacy_eng = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenizer_ger(text):\n",
    "  return [tok.text for tok in spacy_ger.tokenizer(text)]\n",
    "\n",
    "def tokenizer_eng(text):\n",
    "  return [tok.text for tok in spacy_eng.tokenizer(text)]\n",
    "\n",
    "german = Field(tokenize=tokenizer_ger, lower=True, init_token='<sos>', eos_token='<eos>')\n",
    "english = Field(tokenize=tokenizer_eng, lower=True, init_token='<sos>', eos_token='<eos>')\n",
    "\n",
    "train_data, validation_data, test_data = Multi30k.splits(exts=('.de', '.en'), fields=(german, english))\n",
    "\n",
    "german.build_vocab(train_data, max_size=10000, min_freq=2)\n",
    "english.build_vocab(train_data, max_size=10000, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "  def __init__(self, embedding_size, src_vocab_size, trg_vocab_size, src_pad_idx, \n",
    "               num_heads, num_encoder_layers, num_decoder_layers, forward_expansion, \n",
    "               dropout, max_len, device\n",
    "               ):\n",
    "    super(Transformer, self).__init__()\n",
    "    self.src_word_embedding = nn.Embedding(src_vocab_size, embedding_size)\n",
    "    self.src_position_embeddding = nn.Embedding(max_len, embedding_size)\n",
    "    self.trg_word_embedding = nn.Embedding(trg_vocab_size, embedding_size)\n",
    "    self.trg_position_embedding = nn.Embedding(max_len, embedding_size)\n",
    "    self.device = device\n",
    "    self.transformer = nn.Transformer(embedding_size, num_heads, \n",
    "                                      num_encoder_layers=num_encoder_layers, \n",
    "                                      num_decoder_layers=num_decoder_layers,\n",
    "                                      dim_feedforward=forward_expansion,\n",
    "                                      dropout=dropout,\n",
    "                                      )\n",
    "    self.fc_out = nn.Linear(embedding_size, trg_vocab_size)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    self.src_pad_idx = src_pad_idx\n",
    "\n",
    "  def make_src_mask(self, src):\n",
    "    # src shape = (src_len, N)\n",
    "    src_mask = src.transpose(0,1) == self.src_pad_idx\n",
    "    # (N, src_mask)\n",
    "    return src_mask\n",
    "\n",
    "  def forward(self, src, trg):\n",
    "    # Get shapes\n",
    "    src_seq_length, N1 = src.shape\n",
    "    trg_seq_length, N2 = trg.shape\n",
    "    # print(f\"src_seq_length, N1 --> {src_seq_length}, {N1}\")\n",
    "    # print(f\"trg_seq_length, N2 --> {trg_seq_length}, {N2}\")\n",
    "\n",
    "    # Create Positions\n",
    "    src_position = (\n",
    "        torch.arange(0, src_seq_length).unsqueeze(1).expand(src_seq_length, N1)\n",
    "        .to(self.device)\n",
    "    )\n",
    "\n",
    "    trg_position = (\n",
    "        torch.arange(0, trg_seq_length).unsqueeze(1).expand(trg_seq_length, N2)\n",
    "        .to(self.device)\n",
    "    )\n",
    "\n",
    "    # Embed positions into data\n",
    "    embed_src = self.dropout(\n",
    "        (self.src_word_embedding(src) + self.src_position_embeddding(src_position))\n",
    "    )\n",
    "    embed_trg = self.dropout(\n",
    "        (self.trg_word_embedding(trg) + self.trg_position_embedding(trg_position))\n",
    "    )\n",
    "\n",
    "    src_padding_mask = self.make_src_mask(src)\n",
    "    trg_mask = self.transformer.generate_square_subsequent_mask(trg_seq_length).to(\n",
    "        self.device\n",
    "        )\n",
    "    out = self.transformer(\n",
    "        embed_src,\n",
    "        embed_trg,\n",
    "        src_key_padding_mask = src_padding_mask,\n",
    "        tgt_mask=trg_mask,\n",
    "\n",
    "    )\n",
    "    # print(f\"trans out: {out.shape}\")\n",
    "    out = self.fc_out(out)\n",
    "    # print(f\"feed out: {out.shape}\")\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the training phase\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "load_model = False\n",
    "save_model = True\n",
    "\n",
    "# Training hyperparameters\n",
    "num_epochs = 5\n",
    "learning_rate = 3e-4\n",
    "batch_size = 32\n",
    "\n",
    "# Model hyperparameters\n",
    "src_vocab_size = len(german.vocab)\n",
    "trg_vocab_size = len(english.vocab)\n",
    "embedding_size = 512\n",
    "num_heads = 8\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "dropout = 0.1\n",
    "max_len = 100\n",
    "forward_expansion = 2048\n",
    "src_pad_idx = english.vocab.stoi[\"<pad>\"]\n",
    "\n",
    "# Tensorboard for nice plots\n",
    "writer = SummaryWriter(\"runs/loss_plot\")\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train_data, \n",
    "                                                                      validation_data, test_data), batch_size=batch_size, sort_within_batch=True,\n",
    "                                                                      sort_key = lambda x: len(x.src),\n",
    "                                                                      device=device)\n",
    "\n",
    "model = Transformer(\n",
    "    embedding_size,\n",
    "    src_vocab_size, \n",
    "    trg_vocab_size,\n",
    "    src_pad_idx,\n",
    "    num_heads,\n",
    "    num_encoder_layers,\n",
    "    num_decoder_layers,\n",
    "    forward_expansion,\n",
    "    dropout,\n",
    "    max_len,\n",
    "    device,\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "pad_idx = english.vocab.stoi[\"<pad>\"]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 150\n",
    "checkpoint_filename=\"my_checkpoint_trans.pth.tar\"\n",
    "\n",
    "if load_model:\n",
    "  load_checkpoint(torch.load(checkpoint_filename), model, optimizer)\n",
    "\n",
    "sentence = 'Ein Boot mit mehreren Männern wird von einem großen Pferdegespann ans Ufer gezogen..'\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch [{epoch} / {num_epochs}]\")\n",
    "\n",
    "    checkpoint = {'state_dict' : model.state_dict(), 'optimizer':optimizer.state_dict()}\n",
    "    save_checkpoint(checkpoint, filename=checkpoint_filename)\n",
    "\n",
    "    model.eval()\n",
    "    translated_sentence = translate_sentence(model, sentence, german, english, device, max_length=100)\n",
    "    print(f\"Translated example sentence \\n {translated_sentence}\")\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, bacth in enumerate(train_iterator):\n",
    "      inp_data = bacth.src.to(device)\n",
    "      target = bacth.trg.to(device)\n",
    "\n",
    "      output = model(inp_data, target[:-1])\n",
    "      # import sys\n",
    "      # sys.exit()\n",
    "      # output shape: (trg_len, bacth_size, output_dim)\n",
    "\n",
    "      output = output.reshape(-1, output.shape[2])\n",
    "      target = target[1:].reshape(-1)\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      loss = criterion(output, target)\n",
    "\n",
    "      loss.backward()\n",
    "\n",
    "      torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm=1)\n",
    "      optimizer.step()\n",
    "\n",
    "      writer.add_scalar('Training loss', loss, global_step=step)\n",
    "      step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = bleu(test_data, model, german, english, device)\n",
    "print(f\"Bleu score {score*100:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8 (main, Nov  4 2022, 09:21:25) [GCC 12.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
