{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mount Drive Storage - (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Drive\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "path_to_root_dir = \"/content/drive/MyDrive/deep learning folder/WasiuNet\" # You are advised to created a folder in your drive with name `deep learning folder` for everything to work seamlessly or You can change this to your desired folder path\n",
    "\n",
    "if os.path.exists(path_to_root_dir) == False:\n",
    "  os.mkdir(path_to_root_dir)\n",
    "\n",
    "# Change directory to Where you want to save results\n",
    "os.chdir(path_to_root_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ta\n",
    "!pip install nest_asyncio\n",
    "!pip install pytorch-lightning\n",
    "!pip install torchinfo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Classes and functions to preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json, time, sys, aiohttp, torch, cProfile, random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from torch.utils.data import random_split\n",
    "from ast import Return\n",
    "from pytorch_lightning.utilities.model_summary import ModelSummary\n",
    "from torchmetrics.functional.classification import multiclass_f1_score, multiclass_accuracy, multiclass_precision, multiclass_recall\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "from random import randint\n",
    "from collections import OrderedDict\n",
    "from torchinfo import summary\n",
    "\n",
    "np.seterr(divide = 'ignore') \n",
    "\n",
    "class HistoricalData(object):\n",
    "    \"\"\"\n",
    "    This class provides methods for gathering historical price data of a specified\n",
    "    Cryptocurrency between user specified time periods. The class utilises the CoinBase Pro\n",
    "    API to extract historical data, providing a performant method of data extraction.\n",
    "    \n",
    "    Please Note that Historical Rate Data may be incomplete as data is not published when no \n",
    "    ticks are available (Coinbase Pro API Documentation).\n",
    "    :param: ticker: a singular Cryptocurrency ticker. (str)\n",
    "    :param: granularity: the price data frequency in seconds, one of: 60, 300, 900, 3600, 21600, 86400. (int)\n",
    "    :param: start_date: a date string in the format YYYY-MM-DD-HH-MM. (str)\n",
    "    :param: end_date: a date string in the format YYYY-MM-DD-HH-MM,  Default=Now. (str)\n",
    "    :param: verbose: printing during extraction, Default=True. (bool)\n",
    "    :returns: data: a Pandas DataFrame which contains requested cryptocurrency data. (pd.DataFrame)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 ticker,\n",
    "                 granularity,\n",
    "                 start_date,\n",
    "                 end_date=None,\n",
    "                 verbose=True):\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Checking input parameters are in the correct format.\")\n",
    "        if not all(isinstance(v, str) for v in [ticker, start_date]):\n",
    "            raise TypeError(\"The 'ticker' and 'start_date' arguments must be strings or None types.\")\n",
    "        if not isinstance(end_date, (str, type(None))):\n",
    "            raise TypeError(\"The 'end_date' argument must be a string or None type.\")\n",
    "        if not isinstance(verbose, bool):\n",
    "            raise TypeError(\"The 'verbose' argument must be a boolean.\")\n",
    "        if isinstance(granularity, int) is False:\n",
    "            raise TypeError(\"'granularity' must be an integer object.\")\n",
    "        if granularity not in [60, 300, 900, 3600, 21600, 86400]:\n",
    "            raise ValueError(\"'granularity' argument must be one of 60, 300, 900, 3600, 21600, 86400 seconds.\")\n",
    "\n",
    "        if not end_date:\n",
    "            end_date = datetime.today().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "        else:\n",
    "            end_date_datetime = datetime.strptime(end_date, '%Y-%m-%d-%H-%M')\n",
    "            start_date_datetime = datetime.strptime(start_date, '%Y-%m-%d-%H-%M')\n",
    "            while start_date_datetime >= end_date_datetime:\n",
    "                raise ValueError(\"'end_date' argument cannot occur prior to the start_date argument.\")\n",
    "\n",
    "        self.ticker = ticker\n",
    "        self.granularity = granularity\n",
    "        self.start_date = start_date\n",
    "        self.start_date_string = None\n",
    "        self.end_date = end_date\n",
    "        self.end_date_string = None\n",
    "        self.verbose = verbose\n",
    "\n",
    "    async def _ticker_checker(self):\n",
    "        \"\"\"This helper function checks if the ticker is available on the CoinBase Pro API.\"\"\"\n",
    "        if self.verbose:\n",
    "            print(\"Checking if user supplied is available on the CoinBase Pro API.\")\n",
    "\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "\n",
    "              hist_url = \"https://api.pro.coinbase.com/products\".format(\n",
    "                    self.ticker,\n",
    "                    self.start_date_string,\n",
    "                    self.end_date_string,\n",
    "                    self.granularity)\n",
    "              \n",
    "              async with session.get(hist_url) as resp:\n",
    "                tkr_response = resp\n",
    "                text0 = await tkr_response.text()\n",
    "                  # response = await resp.json()\n",
    "              # tkr_response = resp\n",
    "              # tkr_response = await session.get(hist_url)\n",
    "\n",
    "\n",
    "        # tkr_response = requests.get(\"https://api.pro.coinbase.com/products\")\n",
    "        if tkr_response.status in [200, 201, 202, 203, 204]:\n",
    "            if self.verbose:\n",
    "                print('Connected to the CoinBase Pro API.')\n",
    "            \n",
    "            response_data = pd.json_normalize(json.loads(text0))\n",
    "            ticker_list = response_data[\"id\"].tolist()\n",
    "\n",
    "        elif tkr_response.status in [400, 401, 404]:\n",
    "            if self.verbose:\n",
    "                print(\"Status Code: {}, malformed request to the CoinBase Pro API.\".format(tkr_response.status))\n",
    "            # sys.exit()\n",
    "        elif tkr_response.status in [403, 500, 501]:\n",
    "            if self.verbose:\n",
    "                print(\"Status Code: {}, could not connect to the CoinBase Pro API.\".format(tkr_response.status))\n",
    "            # sys.exit()\n",
    "        else:\n",
    "            if self.verbose:\n",
    "                print(\"Status Code: {}, error in connecting to the CoinBase Pro API.\".format(tkr_response.status))\n",
    "            # sys.exit()\n",
    "\n",
    "        if self.ticker in ticker_list:\n",
    "            if self.verbose:\n",
    "                print(\"Ticker '{}' found at the CoinBase Pro API, continuing to extraction.\".format(self.ticker))\n",
    "        else:\n",
    "            raise ValueError(\"\"\"Ticker: '{}' not available through CoinBase Pro API. Please use the Cryptocurrencies \n",
    "            class to identify the correct ticker.\"\"\".format(self.ticker))\n",
    "\n",
    "    def _date_cleaner(self, date_time: (datetime, str)):\n",
    "        \"\"\"This helper function presents the input as a datetime in the API required format.\"\"\"\n",
    "        if not isinstance(date_time, (datetime, str)):\n",
    "            raise TypeError(\"The 'date_time' argument must be a datetime type.\")\n",
    "        if isinstance(date_time, str):\n",
    "            output_date = datetime.strptime(date_time, '%Y-%m-%d-%H-%M').isoformat()\n",
    "        else:\n",
    "            output_date = date_time.strftime(\"%Y-%m-%d, %H:%M:%S\")\n",
    "            output_date = output_date[:10] + 'T' + output_date[12:]\n",
    "        return output_date\n",
    "\n",
    "    async def multiple_data_retrieve(self, i,start,request_volume,max_per_mssg):\n",
    "      provisional_start = start + timedelta(0, i * (self.granularity * max_per_mssg))\n",
    "      provisional_start = self._date_cleaner(provisional_start)\n",
    "      provisional_end = start + timedelta(0, (i + 1) * (self.granularity * max_per_mssg))\n",
    "      provisional_end = self._date_cleaner(provisional_end)\n",
    "      if self.verbose:\n",
    "        print(\"Provisional Start: {}\".format(provisional_start))\n",
    "        print(\"Provisional End: {}\".format(provisional_end))\n",
    "      # response = requests.get(\n",
    "      #     \"https://api.pro.coinbase.com/products/{0}/candles?start={1}&end={2}&granularity={3}\".format(\n",
    "      #         self.ticker,\n",
    "      #         provisional_start,\n",
    "      #         provisional_end,\n",
    "      #         self.granularity))\n",
    "      # time.sleep(randint(0, 5))\n",
    "      async with aiohttp.ClientSession() as session:\n",
    "        hist_url = \"https://api.pro.coinbase.com/products/{0}/candles?start={1}&end={2}&granularity={3}\".format(\n",
    "              self.ticker,\n",
    "              provisional_start,\n",
    "              provisional_end,\n",
    "              self.granularity)\n",
    "        # time.sleep(randint(1,5))\n",
    "        async with session.get(hist_url) as resp:\n",
    "          response = resp\n",
    "          text2 = await response.text()\n",
    "        # response = await resp.json()\n",
    "          # response = resp\n",
    "        # response = await session.get(hist_url)\n",
    "\n",
    "          if response.status in [200, 201, 202, 203, 204]:\n",
    "              if self.verbose:\n",
    "                  print('Data for chunk {} of {} extracted'.format(i+1,\n",
    "                                                                    (int(request_volume / max_per_mssg) + 1)))\n",
    "              # text2 = await response.text()\n",
    "              dataset = pd.DataFrame(json.loads(text2), columns=[\"time\", \"low\", \"high\", \"open\", \"close\", \"volume\"])\n",
    "              # dataset.columns = [\"time\", \"low\", \"high\", \"open\", \"close\", \"volume\"]\n",
    "              if not dataset.empty:\n",
    "                  # data = data.append(dataset)\n",
    "                  dataset[\"time\"] = pd.to_datetime(dataset[\"time\"], unit='s')\n",
    "                  return dataset\n",
    "              else:\n",
    "                  print(\"\"\"CoinBase Pro API did not have available data for '{}' beginning at {}.  \n",
    "                  Trying a later date:'{}'\"\"\".format(self.ticker,\n",
    "                                                      self.start_date,\n",
    "                                                      provisional_start))\n",
    "                  # If no data is returned, return zero instead\n",
    "                  d_inx = pd.date_range(start=provisional_start, end=provisional_end, freq=f\"{int(self.granularity/60)}\"+\"min\")\n",
    "                  dataset = pd.DataFrame(np.array(np.zeros((d_inx.shape[0], 6))), columns=[\"time\", \"low\", \"high\", \"open\", \"close\", \"volume\"])\n",
    "                  dataset[\"time\"] = d_inx\n",
    "                  # print(dataset)\n",
    "                  return  dataset\n",
    "          elif response.status in [400, 401, 404]:\n",
    "              if self.verbose:\n",
    "                  print(\n",
    "                      \"Status Code: {}, malformed request to the CoinBase Pro API.\".format(response.status))\n",
    "              # sys.exit()\n",
    "          elif response.status in [403, 500, 501]:\n",
    "              if self.verbose:\n",
    "                  print(\n",
    "                      \"Status Code: {}, could not connect to the CoinBase Pro API.\".format(response.status))\n",
    "              # sys.exit()\n",
    "          else:\n",
    "              if self.verbose:\n",
    "                  print(\"Status Code: {}, error in connecting to the CoinBase Pro API.\".format(\n",
    "                      response.status))\n",
    "              # sys.exit()\n",
    "\n",
    "    async def retrieve_data(self):\n",
    "        \"\"\"This function returns the data.\"\"\"\n",
    "        if self.verbose:\n",
    "            print(\"Formatting Dates.\")\n",
    "\n",
    "        await self._ticker_checker()\n",
    "        self.start_date_string = self._date_cleaner(self.start_date)\n",
    "        self.end_date_string = self._date_cleaner(self.end_date)\n",
    "        start = datetime.strptime(self.start_date, \"%Y-%m-%d-%H-%M\")\n",
    "        end = datetime.strptime(self.end_date, \"%Y-%m-%d-%H-%M\")\n",
    "        request_volume = abs((end - start).total_seconds()) / self.granularity\n",
    "        # request_volume = 1000 # This is for test purpose, revert to previous code\n",
    "\n",
    "        if request_volume <= -1: #(The original value here is 300)\n",
    "            # response = requests.get(\n",
    "            #     \"https://api.pro.coinbase.com/products/{0}/candles?start={1}&end={2}&granularity={3}\".format(\n",
    "            #         self.ticker,\n",
    "            #         self.start_date_string,\n",
    "            #         self.end_date_string,\n",
    "            #         self.granularity))\n",
    "            async with aiohttp.ClientSession() as session:\n",
    "\n",
    "              hist_url = \"https://api.pro.coinbase.com/products/{0}/candles?start={1}&end={2}&granularity={3}\".format(\n",
    "                    self.ticker,\n",
    "                    self.start_date_string,\n",
    "                    self.end_date_string,\n",
    "                    self.granularity)\n",
    "              \n",
    "              async with session.get(hist_url) as resp:\n",
    "                response = resp\n",
    "                text = await response.text()\n",
    "                  # response = await resp.json()\n",
    "                  # response = resp\n",
    "                  # print(pokemon['name'])\n",
    "              # response = await session.get(hist_url)\n",
    "            if response.status in [200, 201, 202, 203, 204]:\n",
    "                if self.verbose:\n",
    "                    print('Retrieved Data from Coinbase Pro API.')\n",
    "                # text = await response.text()\n",
    "                data = pd.DataFrame(json.loads(text))\n",
    "                data.columns = [\"time\", \"low\", \"high\", \"open\", \"close\", \"volume\"]\n",
    "                data[\"time\"] = pd.to_datetime(data[\"time\"], unit='s')\n",
    "                data = data[data['time'].between(start, end)]\n",
    "                data.set_index(\"time\", drop=True, inplace=True)\n",
    "                data.sort_index(ascending=True, inplace=True)\n",
    "                data.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "                if self.verbose:\n",
    "                    print('Returning data.')\n",
    "                return data\n",
    "            elif response.status in [400, 401, 404]:\n",
    "                if self.verbose:\n",
    "                    print(\"Status Code: {}, malformed request to the CoinBase Pro API.\".format(response.status))\n",
    "                # sys.exit()\n",
    "            elif response.status in [403, 500, 501]:\n",
    "                if self.verbose:\n",
    "                    print(\"Status Code: {}, could not connect to the CoinBase Pro API.\".format(response.status))\n",
    "                # sys.exit()\n",
    "            else:\n",
    "                if self.verbose:\n",
    "                    print(\"Status Code: {}, error in connecting to the CoinBase Pro API.\".format(response.status))\n",
    "                # sys.exit()\n",
    "        else:\n",
    "            # The api limit:\n",
    "            max_per_mssg = 300\n",
    "            data = pd.DataFrame()\n",
    "            chunk_size = 5\n",
    "            total_idx = int(request_volume / max_per_mssg) + 1\n",
    "            all_idx = list(range(total_idx))\n",
    "            for idx in range(0,total_idx, chunk_size):\n",
    "              loop = True\n",
    "              while loop:\n",
    "                results = []\n",
    "                if self.verbose:print(f\"{idx}\")\n",
    "                tasks = [self.multiple_data_retrieve(i,start, request_volume, max_per_mssg) for i in all_idx[idx:idx+chunk_size]]\n",
    "                results = await asyncio.gather(*tasks)\n",
    "                for dataset in results:\n",
    "                  if isinstance(dataset, type(None)):\n",
    "                    loop = True\n",
    "                    time.sleep(5)\n",
    "                    break\n",
    "                  loop = False\n",
    "\n",
    "              for dataset in results:\n",
    "                  if not dataset.empty:\n",
    "                    data = data.append(dataset)\n",
    "              time.sleep(randint(0, 2))\n",
    "            # print(f\"data --> {data}\")\n",
    "            data.columns = [\"time\", \"low\", \"high\", \"open\", \"close\", \"volume\"]\n",
    "            data.drop_duplicates('time',keep='last', inplace=True)\n",
    "\n",
    "            data[\"time\"] = pd.to_datetime(data[\"time\"], unit='s')\n",
    "            data = data[data['time'].between(start, end)]\n",
    "            data.set_index(\"time\", drop=True, inplace=True)\n",
    "            data.sort_index(ascending=True, inplace=True)\n",
    "            # data.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "            return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import required libraries\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset\n",
    "# from Historic_Crypto import HistoricalData\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch as tch\n",
    "import ta\n",
    "import asyncio\n",
    "import time\n",
    "\n",
    "class DatasetBaseBackend(Dataset):\n",
    "  def __init__(self, asset, resolution, start_date, end_date,glob_time_step_forwards=7,glob_time_step_backwards=None, fea_output_per_data_slice=30, fea_data_slice=7, technical_analysis_config=None,**kwarg):\n",
    "    \"\"\"\n",
    "      This method inits the Custom dataset for the trader.\n",
    "      This method has all basic methods required to extend this\n",
    "      project to a new dataset\n",
    "      asset: The  Exchage  pair to pull e.g BTC-USD (This is a backend specific \n",
    "             parameter, So read backend doc to know parameter format)\n",
    "\n",
    "      resolution: This is the timeframe between data, e.g 1m, 2m, 60, 90 \n",
    "             (This is a backend specific parameter, So read backend doc to know \n",
    "             parameter format)\n",
    "\n",
    "      start_date: Date for dataset object to start pulling data from. Expected \n",
    "             format '%Y-%m-%d-%H-%M-%S'\n",
    "\n",
    "      end_date: Date for dataset object to stop pulling data. Expected format \n",
    "             '%Y-%m-%d-%H-%M-%S'\n",
    "\n",
    "      glob_time_step_forwards: Timestep forward, e.g take 7 steps forward from \n",
    "             start_date. default value is 7. This is part of the training \n",
    "             stategy as it is used in the __getitem__ method.\n",
    "\n",
    "      glob_time_step_backwards: Timestep backward, e.g take 448 steps forward \n",
    "             from start_date. default value is 448. This is part of the training \n",
    "             stategy as it is used in the __getitem__ method.\n",
    "\n",
    "      technical_analysis_config: This is a list of dictionaries where each dictionary \n",
    "            are in this format {\"ta_func_name\":\"RSI\", ta_func_config:{}}.\n",
    "            Do note for the ta_func_config, user needs to read function's documentation\n",
    "            to know what hyper parameter needs to go in e.g for RSI hyper parameters are\n",
    "            window: int=14, fillna: bool=False, while close is not a hyper-parameter.\n",
    "          \n",
    "      fea_data_slice: This parameter specifies the amount of data slices a single data point\n",
    "            should has e.g if i index row 0, output should be 7 different data slices with the same\n",
    "            fixed row length.\n",
    "\n",
    "      fea_output_per_data_slice: This parameter specifies the number of data points to be\n",
    "            contained in each data slice. E.g one data point contains 7 data slices and each data \n",
    "            slice has 30 sub data points\n",
    "    \"\"\"\n",
    "    self.start_date = datetime.strptime(start_date, '%Y-%m-%d-%H-%M-%S')\n",
    "    self.end_date = datetime.strptime(end_date, '%Y-%m-%d-%H-%M-%S')\n",
    "    self.asset = asset\n",
    "    self.fea_output_per_data_slice=fea_output_per_data_slice\n",
    "    self.fea_data_slice=fea_data_slice\n",
    "    self.resolution = resolution\n",
    "    if isinstance(self.resolution, (int, float)):\n",
    "      self.normal_step = self.resolution\n",
    "    else:\n",
    "      # TODO: map string and convert string to respective integer value in minutes\n",
    "      #        this will be used in the getitem method to get ith index data point.\n",
    "      pass\n",
    "    self.glob_time_step_forwards = glob_time_step_forwards\n",
    "    # self.glob_time_step_backwards = glob_time_step_backwards\n",
    "    self.glob_time_step_backwards = (self.backward_step_func(fea_data_slice)*fea_output_per_data_slice)*5 if glob_time_step_backwards is None else glob_time_step_backwards\n",
    "    # print(self.glob_time_step_backwards)\n",
    "    self.set_available_technical_analysis_functions()\n",
    "    self.set_technical_analysis_for_dataloader(technical_analysis_config)\n",
    "    self.data_scaled = False\n",
    "    self._extra_direction_fields = [\"direction_-1_conf\",\"direction_0_conf\",\"direction_1_conf\"]\n",
    "    self._required_base_features = ['close','high','low','open','volume']\n",
    "    self._required_base_target = ['close']\n",
    "    self.squeeze_forward = kwarg.get('squeeze_forward',False)\n",
    "    self.init_preload(**kwarg)\n",
    "\n",
    "  def convert_date_from_backend_format(self,date, format=None):\n",
    "    \"\"\"\n",
    "      This method is used to convert from custom dataset datetime format to \n",
    "      backend format, If data is datetime convert to backend string else if \n",
    "      data is string leave as it is.\n",
    "\n",
    "      date: a datetime string\n",
    "      format: a datetime string format e.g '%Y-%m-%d-%H-%M-%S'\n",
    "    \"\"\"\n",
    "    return datetime.strptime(date, format)\n",
    "\n",
    "  def convert_date_to_backend_format(self, date, format='%Y-%m-%d-%H-%M-%S'):\n",
    "    \"\"\"\n",
    "      This method is used to convert date from str to the required date format \n",
    "      for the dataloader\n",
    "\n",
    "      date: a datetime object,\n",
    "      format: the format in which the  date is  in e.g '%Y-%m-%d-%H-%M-%S'\n",
    "    \"\"\"\n",
    "    if isinstance(date, datetime):\n",
    "      assert isinstance(format, str), \"format should be in string format, please check\"\n",
    "      return date.strftime(format)\n",
    "    else:\n",
    "      return date\n",
    "\n",
    "  def add_custom_targets(self):\n",
    "    raise NotImplementedError(\"Sub-class Should implement this method\")\n",
    "\n",
    "  def add_all_targets(self,data_in_required_format_with_ta):\n",
    "    raise NotImplementedError(\"Sub-class Should implement this method\")\n",
    "    \n",
    "  def get_backend(self):\n",
    "    \"\"\"\n",
    "      This method returns the current backend used to access the crypto data\n",
    "    \"\"\"\n",
    "    return self.backend\n",
    "\n",
    "  def get_required_base_index_name(self):\n",
    "    \"\"\"\n",
    "      These are the required index name to be able to use the inner methods.\n",
    "    \"\"\"\n",
    "    return ['time']\n",
    "\n",
    "  def get_extra_direction_fields(self):\n",
    "    \"\"\"\n",
    "      These are fields that were added during the process of data processing.\n",
    "    \"\"\"\n",
    "    return self._extra_direction_fields\n",
    "\n",
    "  def get_required_base_features(self):\n",
    "    \"\"\"\n",
    "      These are the required features to be able to use the inner methods.\n",
    "    \"\"\"\n",
    "    return self._required_base_features\n",
    "\n",
    "  def get_required_base_target(self):\n",
    "    \"\"\"\n",
    "      These are the required target to be able to use the inner methods.\n",
    "    \"\"\"\n",
    "    return self._required_base_target\n",
    "\n",
    "  def get_supported_backends(self):\n",
    "    \"\"\"\n",
    "      This method returns a list of strings of supported backends used to access \n",
    "      data\n",
    "    \"\"\"\n",
    "    return ['historicCryptoBackend']\n",
    "\n",
    "  # def backend_get_all_raw(self,asset=None, resolution=None, start_date=None, end_date=None):\n",
    "  #   \"\"\"\n",
    "  #     Implement this function from each backend\n",
    "  #   \"\"\"\n",
    "  #   raise NotImplementedError(\"Please implement this method in your dataset to return the required datapoints\")\n",
    "\n",
    "  def get_all_raw(self, asset=None, resolution=None, start_date=None, end_date=None):\n",
    "    \"\"\"\n",
    "      This method returns the raw dataframe for the current backend in the specified \n",
    "      def backend_get_all_raw(self,asset=None, resolution=None, start_date=None, end_date=None)\n",
    "      timeframe. This function implicitly calls the backend_get_all_raw function to run the \n",
    "      backend specific implementation. \n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "      return self.backend_get_all_raw(asset=asset, resolution=resolution, start_date=start_date, end_date=end_date)\n",
    "    except Exception as e:\n",
    "      raise Exception(f\"Error {e} occured while getting data from backend_get_all_raw method\")\n",
    "\n",
    "  def set_backend(self, name):\n",
    "    \"\"\"\n",
    "      This method is used to set the  current backend used to access crypto data\n",
    "    \"\"\"\n",
    "    self.backend = name \n",
    "\n",
    "  def __len__(self):\n",
    "    \"\"\"\n",
    "      Used to get the supposed length  of the data set assuming all columns were \n",
    "      loaded and market traded every seconds and minutes of the day.\n",
    "    \"\"\"\n",
    "    num_of_sec_diff = self.end_date - self.start_date\n",
    "    num_of_expected_samps = int(num_of_sec_diff.total_seconds()/60)\n",
    "    return num_of_expected_samps\n",
    "\n",
    "  def expand_dset_to_time(self,data, idx, outs, steps,forward,include_current_idx):\n",
    "    \"\"\"\n",
    "      This method helps slice the data into current output expected\n",
    "    \"\"\"\n",
    "      # backward pass\n",
    "    outs = outs + 1 if not include_current_idx else outs\n",
    "    if not forward:\n",
    "        new_data = data.iloc[(idx-(outs*steps))+steps:idx+include_current_idx:steps]\n",
    "\n",
    "    # forward pass\n",
    "    if forward:\n",
    "        include_current_id_for = int(not include_current_idx) if forward else int(include_current_idx)\n",
    "        add_to_steps = 1 if steps<=1 else 0\n",
    "        new_data = data.iloc[(idx+steps)-include_current_idx:(idx+(outs*steps))+include_current_idx:steps]\n",
    "\n",
    "    return new_data.copy()\n",
    "\n",
    "\n",
    "  def get_available_technical_analysis_functions(self, return_func=True):\n",
    "    \"\"\"\n",
    "      This returns all the names used to access all technical analysis that this \n",
    "      dataloaded currently handles.\n",
    "    \"\"\"\n",
    "    assert self.set_available_technical_analysis_functions_flag, \"Call the `set_available_technical_analysis_functions` method to initialize the technical functoins\"\n",
    "    if return_func:\n",
    "      return self.available_technical_analysis_functions\n",
    "    return self.available_technical_analysis_functions.keys()\n",
    "\n",
    "  def set_available_technical_analysis_functions(self):\n",
    "    self.available_technical_analysis_functions = {\n",
    "                                                   \"SMA\" : {\"name\":\"SMAIndicator\",'func':ta.trend.SMAIndicator,'data_cols':['close']},\n",
    "\n",
    "                                                   \"RSI\" : {\n",
    "                                                            \"name\":\"RSI Momentum Indicator\",\n",
    "                                                            'func':ta.momentum.RSIIndicator,'data_cols':['close']},\n",
    "                                                   \"STC\" : {\n",
    "                                                            \"name\" : \"STCIndicator\", 'func':ta.trend.STCIndicator,'data_cols':['close']\n",
    "                                                   },\n",
    "                                                   \n",
    "                                                   \"CCI\" : {\n",
    "                                                            \"name\":\"Commodity Channel Index\",\n",
    "                                                            'func':ta.trend.CCIIndicator,'data_cols':['close']},\n",
    "                                                    \n",
    "                                                   \"AO\"  : {\n",
    "                                                            \"name\":\"Awesome Oscillator\",\n",
    "                                                            'func':ta.momentum.AwesomeOscillatorIndicator,'data_cols':['close']},\n",
    "                                                   \n",
    "                                                   \"MACD\": {\n",
    "                                                            \"name\":\"Moving Average Convergence Divergence\",\n",
    "                                                            'func':ta.trend.MACD,'data_cols':['close']},\n",
    "                                                   \n",
    "                                                   \"ATR\" : {\n",
    "                                                            \"name\":\"Average True Range\",\n",
    "                                                            'func':ta.volatility.AverageTrueRange,'data_cols':['close']},\n",
    "                                                   \n",
    "                                                   \"OBVI\" : {\n",
    "                                                            \"name\":\"On Balance Volume Indicator\",\n",
    "                                                            'func':ta.volume.OnBalanceVolumeIndicator,'data_cols':['close']},\n",
    "                                                   \n",
    "                                                   \"KAMA\" : {\n",
    "                                                              \"name\":\"KAMA Indicator\",\n",
    "                                                              'func':ta.momentum.KAMAIndicator,'data_cols':['close']},\n",
    "                                                   \n",
    "                                                   \"ADX\"  : {\n",
    "                                                            \"name\":\"Directional Movement\",\n",
    "                                                            'func':ta.trend.ADXIndicator,'data_cols':['close']},\n",
    "                                                   \n",
    "                                                   \"STOCH\":{\"name\":\"Stochastic Oscillation\",\n",
    "                                                            'func':ta.momentum.StochasticOscillator,'data_cols':['close']},\n",
    "\n",
    "                                                   \"STOCHRSI\":{\"name\":\"Stochastic RSI\",\n",
    "                                                               'func':ta.momentum.StochRSIIndicator,'data_cols':['close']},\n",
    "                                                   \n",
    "                                                   \"WILLP\":{\"name\":\"William's %R\",\n",
    "                                                            'func':ta.momentum.WilliamsRIndicator,'data_cols':['close']},\n",
    "                                                   \"ARN\":{\"name\":\"AroonIndicator\",'func':ta.trend.AroonIndicator,'data_cols':['close']},\n",
    "                                                   \"MASSIDX\":{\"name\":\"MassIndex\",'func':ta.trend.MassIndex,'data_cols':['close']},\n",
    "                                                   \"PSAR\":{\"name\":\"PSARIndicator\",'func':ta.trend.PSARIndicator,'data_cols':['close']}\n",
    "                                                   }\n",
    "    self.set_available_technical_analysis_functions_flag =  True\n",
    "\n",
    "  def get_current_technical_analysis_config_for_dataloader(self):\n",
    "    return self.technical_analysis_for_dataloader\n",
    "\n",
    "  def set_technical_analysis_for_dataloader(self, config):\n",
    "    self.technical_analysis_for_dataloader = config\n",
    "\n",
    "  def extract_final_ta_level_data(self, final_ta_data_obj):\n",
    "    \"\"\"\n",
    "      This method is ta specific, to implement the final data extract that the ta returns\n",
    "    \"\"\"\n",
    "    if isinstance(final_ta_data_obj, ta.trend.SMAIndicator):\n",
    "      return final_ta_data_obj.sma_indicator()\n",
    "    if isinstance(final_ta_data_obj, ta.momentum.RSIIndicator):\n",
    "      return final_ta_data_obj.rsi()\n",
    "    if isinstance(final_ta_data_obj, ta.trend.STCIndicator):\n",
    "      return final_ta_data_obj.stc()\n",
    "    if isinstance(final_ta_data_obj, ta.trend.CCIIndicator):\n",
    "      return final_ta_data_obj.cci()\n",
    "    if isinstance(final_ta_data_obj, ta.momentum.AwesomeOscillatorIndicator):\n",
    "      return final_ta_data_obj.awesome_oscillator()\n",
    "\n",
    "  def get_all_ta_cofig_output_columns(self):\n",
    "    cols = set()\n",
    "    ta_config = self.get_current_technical_analysis_config_for_dataloader()\n",
    "    for each_ta in ta_config: cols.add(self.ta_to_col_name(each_ta))\n",
    "    return list(cols)\n",
    "\n",
    "  def ta_to_col_name(self, each_ta):\n",
    "    \"\"\"\n",
    "    This function takes in the technical config for each ta dict and the hyper \n",
    "    parameter and returns the column name to represent that ta in the data frame\n",
    "\n",
    "    ta_func_name: ta function name to init that ta for dataframe\n",
    "    col_hyper: the hyperparamters in the ta config\n",
    "    \"\"\"\n",
    "    col_hyper = each_ta.get('ta_func_config')\n",
    "    ta_func_name = each_ta.get('ta_func_name')\n",
    "    return \"-\".join([\"ta_func:\"+ta_func_name]+[str(ta_key)+\":\"+str(ta_value) for ta_key, ta_value in col_hyper.items()])\n",
    "\n",
    "  def add_technical_analysis_to_dataset(self,data_in_required_format):\n",
    "    \"\"\"\n",
    "      This method is responsible for taking in the raw data in a required format, \n",
    "      loops through the ta config and adds the ta to the dataset, while also storing \n",
    "      their column names.\n",
    "    \"\"\"\n",
    "    # Get the required fields\n",
    "    # Check for errors in features names, target and index name provided\n",
    "    try:\n",
    "      # print(f\"data_in_required_format: {data_in_required_format}\")\n",
    "      assert isinstance(data_in_required_format, pd.DataFrame), \"Please `data_in_required_format` should be of type pd.DataFrame\"\n",
    "      assert set(data_in_required_format.columns) == set(self.get_required_base_features()+ self.get_required_base_target()), \"The feature and target provided does not match the required fields, please run self.get_required_base_features() and self.get_required_base_target()\"\n",
    "      assert set([data_in_required_format.index.name]) == set(self.get_required_base_index_name()), \"The index name provided does not match the required index name, please run self.get_required_base_index_name() to see the required index name\"\n",
    "    except Exception as e:\n",
    "      raise ValueError(f\"Error {e} happened when trying to check the quality of data provided\")\n",
    "\n",
    "    # parse the technical analysis config provided to add the new columns to dataframe.\n",
    "    ta_config =  self.get_current_technical_analysis_config_for_dataloader() # {'ta_func_name': 'SMA', 'ta_func_config': {'window': 7, 'fillna': True}}\n",
    "    system_ta =   self.get_available_technical_analysis_functions() # \"SMA\" : {\"name\":\"SMAIndicator\",'func':ta.trend.SMAIndicator}\n",
    "    # This is where we dynamically pick the parameters from the config and load into the ta class\n",
    "    for each_ta in ta_config:\n",
    "      ta_function = system_ta.get(each_ta.get('ta_func_name'))\n",
    "      col_param = {each_param : data_in_required_format[each_param] for each_param in ta_function.get('data_cols')} \n",
    "      col_hyper = each_ta.get('ta_func_config')\n",
    "      total_col_param = {**col_param, **col_hyper}\n",
    "      ta_obj_result = ta_function.get('func')(**total_col_param)\n",
    "\n",
    "      # Save each new ta extract with the config name and place in the data format to return\n",
    "      # ta_name = \"-\".join([\"ta_func:\"+c]+[str(ta_key)+\":\"+str(ta_value) for ta_key, ta_value in col_hyper.items()])\n",
    "      ta_name = self.ta_to_col_name(each_ta)\n",
    "      data_in_required_format[ta_name] = self.extract_final_ta_level_data(ta_obj_result)\n",
    "    data_in_required_format = data_in_required_format.fillna(0)\n",
    "    data_in_required_format = data_in_required_format[set(self.get_required_base_features()+self.get_required_base_target()+self.get_all_ta_cofig_output_columns())]\n",
    "    # Y = data_in_required_format[self.return_all_output_col()]\n",
    "    data_in_required_format = self.replace_inf(data_in_required_format)\n",
    "    return data_in_required_format\n",
    "\n",
    "  def return_all_output_col(self, get_required_base_features=True, get_required_base_target=True, get_all_ta_cofig_output_columns=True, get_extra_direction_fields=True):\n",
    "    \"\"\"\n",
    "      This method returns all the expected columns in a dataset\n",
    "    \"\"\"\n",
    "    cols = set()\n",
    "    if get_required_base_features:\n",
    "      cols = cols.union(self.get_required_base_features())\n",
    "    if get_required_base_target:\n",
    "      cols = cols.union(self.get_required_base_target())\n",
    "    if get_all_ta_cofig_output_columns:\n",
    "      cols = cols.union(self.get_all_ta_cofig_output_columns())\n",
    "    if get_extra_direction_fields:\n",
    "      cols = cols.union(self.get_extra_direction_fields())\n",
    "    \n",
    "    return sorted(list(cols))\n",
    "\n",
    "  def return_all_output_col_as_dict(self, **kwarg):\n",
    "    return_all_output_col_as_dict = self.return_all_output_col(**kwarg)\n",
    "    return {key : value for value, key in enumerate(return_all_output_col_as_dict)}\n",
    "                                                    \n",
    "  def get_idx_start_back_end_date(self,idx, forward_retry_value=0):\n",
    "    \"\"\"\n",
    "      This method converts int index into timespace\n",
    "      in the spcified range, where idx zero is the first\n",
    "      time space in the specified range.\n",
    "      It returns idx_start_date which specifies where to start\n",
    "      pulling data from for a single index, It also returns \n",
    "      idx_backwards_date which is by how far the data pull should go back in time\n",
    "      from the start date and lastly it returns idx_end_date which is by how much \n",
    "      data should go forward in time from the start date.\n",
    "\n",
    "      forward_retry_value: This helps prevent cases when api package does not give an output,\n",
    "      so we can manually expand the end date and rety the pull.\n",
    "    \"\"\"\n",
    "    # Date we are currently interested in (This is the specific day we would like to index out)\n",
    "    idx_start_date = self.start_date + pd.Timedelta(seconds=idx*self.normal_step)\n",
    "    # print(\"Starting date \",idx_start_date,\"--\",self.start_date,\"--\", pd.Timedelta(seconds=idx*self.normal_step))\n",
    "\n",
    "    # print(self.glob_time_step_backwards, self.normal_step, self.glob_time_step_backwards * self.normal_step)\n",
    "    # Date to start pulling data from (Given the structure of the output of the data and it's partition, this is how far back we would like to go back in time))\n",
    "    idx_backwards_date = idx_start_date - pd.Timedelta(seconds=(self.glob_time_step_backwards)*self.normal_step)# - pd.Timedelta(seconds=3000*self.normal_step)\n",
    "    # print(\"Backward date \",\"--\",self.glob_time_step_backwards,\"--\",self.normal_step,\"--\", idx_backwards_date,\"--\",idx_start_date,\"--\", pd.Timedelta(seconds=self.glob_time_step_backwards*self.normal_step))\n",
    "    \n",
    "    # Date to stop pulling data from (Given the structure of the output of the data and it's partition,  this is how further into the future we would like to go in time)\n",
    "    idx_end_date = idx_start_date + pd.Timedelta(seconds=forward_retry_value+self.glob_time_step_forwards*(self.normal_step**2))# + pd.Timedelta(seconds=3000*self.normal_step)\n",
    "    # print(\"End date \",\"--\",self.glob_time_step_forwards,\"--\",self.normal_step,\"--\",idx_end_date,\"--\", idx_start_date,\"--\", pd.Timedelta(seconds=self.glob_time_step_forwards*self.normal_step))\n",
    "\n",
    "    return idx_start_date, idx_backwards_date, idx_end_date\n",
    "\n",
    "  def backward_step_func(self,i):\n",
    "    \"\"\"\n",
    "      This is the backward slicing step for each slice in a one data index.\n",
    "      Say index zero give 7 different slices of data as output for train.\n",
    "      First slice will have backward step of (1+0)**2, second slice will \n",
    "      have a backward step of (1+1)**2 and so on till it gets to the final\n",
    "      slice 6 which will give (1+6)**2.\n",
    "    \"\"\"\n",
    "    step_func = (1+i)**2\n",
    "    # print(f\"step func --> {step_func}\")\n",
    "    return step_func\n",
    "\n",
    "  def search_time_index(self, data, time):\n",
    "    \"\"\"\n",
    "      This functionality search for the\n",
    "    \"\"\"\n",
    "    return data.index.searchsorted(time)\n",
    "\n",
    "  def get_multi_slice_backward_output(self, X, current_level_index):\n",
    "    # print(self.backward_step_func(self.fea_data_slice)*self.fea_output_per_data_slice , X.shape[0])\n",
    "    assert (self.backward_step_func(self.fea_data_slice)*self.fea_output_per_data_slice) < X.shape[0], f\"number of rows {X.shape[0]} less than required which should be at least {self.backward_step_func(self.fea_data_slice)*self.fea_output_per_data_slice}, Please check your glob_time_step_backwards parameter\"\n",
    "    # if to_numpy:\n",
    "    #   all_inputs = [self.expand_dset_to_time(data=X, idx=current_level_index, outs=self.fea_output_per_data_slice, steps=self.backward_step_func(i),forward=0,include_current_idx=1).to_numpy() for i in range(self.fea_data_slice)]\n",
    "    #   all_inputs = np.stack(all_inputs, axis=0)\n",
    "    #   all_inputs = np.expand_dims(all_inputs, axis=0)\n",
    "    # else:\n",
    "    all_inputs = [self.expand_dset_to_time(data=X, idx=current_level_index, outs=self.fea_output_per_data_slice, steps=self.backward_step_func(i),forward=0,include_current_idx=1) for i in range(self.fea_data_slice)]\n",
    "\n",
    "    return all_inputs\n",
    "\n",
    "  def get_single_slice_forward_output(self, Y, current_level_index):\n",
    "    single_output = self.expand_dset_to_time(data=Y, idx=current_level_index, outs=1, steps=self.glob_time_step_forwards,forward=1,include_current_idx=0)\n",
    "    return single_output\n",
    "\n",
    "  def get_multi_slice_forward_output(self, Y,current_level_index, to_numpy=True):\n",
    "    # if to_numpy:\n",
    "    #   single_output = self.expand_dset_to_time(data=Y, idx=current_level_index, outs=1, steps=self.glob_time_step_forwards,forward=1,include_current_idx=0).to_numpy()\n",
    "    # else:\n",
    "    single_output = self.expand_dset_to_time(data=Y, idx=current_level_index, outs=self.glob_time_step_forwards, steps=1,forward=1,include_current_idx=1)\n",
    "    return single_output\n",
    "\n",
    "  def retry_func(self, curr_val):\n",
    "    return ((curr_val+self.normal_step)**curr_val)+1\n",
    "\n",
    "  def async_get_data_by_idx(self, idx, process_data=True, add_ta=True, apply_nat_log=False, fill_na=True, cal_pct_chg=False, to_numpy=True):\n",
    "    if idx < 0: idx = len(self) + idx\n",
    "    if idx >= len(self) or idx < 0: raise IndexError(\"list index out of range\")\n",
    "    # print(f\"started task {idx}\")\n",
    "    retry_count = 0\n",
    "    max_retry = 4\n",
    "\n",
    "    # while retry_count < max_retry:\n",
    "    #   # Get all data in past 3000 minutes and future 3000 minutes\n",
    "    #   try:\n",
    "    #     print(\"in get all\")\n",
    "    #     idx_start_date, idx_backwards_date, idx_end_date = self.get_idx_start_back_end_date(idx, forward_retry_value=self.retry_func(retry_count)) #convert the index into actual start, backward and enddate \n",
    "    #     data_in_required_format = self.get_all_raw(self.asset, self.resolution, idx_backwards_date, idx_end_date)\n",
    "    #     break\n",
    "    #   except Exception as e:\n",
    "    #     print(e)\n",
    "    #   retry_count += 1\n",
    "\n",
    "    idx_start_date, idx_backwards_date, idx_end_date = self.get_idx_start_back_end_date(idx, forward_retry_value=1) #convert the index into actual start, backward and enddate \n",
    "    # print(f\"idx_start_date: {idx_start_date}\")\n",
    "    try:\n",
    "      data_in_required_format = self.get_all_raw(self.asset, self.resolution, idx_backwards_date, idx_end_date)\n",
    "    except Exception as e:\n",
    "      raise Exception(f\" Error {e} occured while getting record for data point with id {idx}\")\n",
    "\n",
    "    data_in_required_format = self.process_data_raw(data_in_required_format,add_ta=add_ta, apply_nat_log=apply_nat_log, fill_na=fill_na, cal_pct_chg=cal_pct_chg)\n",
    "    data_in_required_format = data_in_required_format[self.return_all_output_col(get_required_base_features=True, get_required_base_target=True, get_all_ta_cofig_output_columns=add_ta, get_extra_direction_fields=cal_pct_chg)]\n",
    "    # # return base_data\n",
    "    # if process_data:\n",
    "    #   data_in_required_format = self.process_data_raw(data_in_required_format,add_ta=add_ta, apply_nat_log=apply_nat_log, fill_na=fill_na, cal_pct_chg=cal_pct_chg)\n",
    "    #   data_in_required_format = data_in_required_format[self.return_all_output_col(get_required_base_features=True, get_required_base_target=True, get_all_ta_cofig_output_columns=add_ta, get_extra_direction_fields=False)]\n",
    "    # else:\n",
    "    #   data_in_required_format = data_in_required_format[self.return_all_output_col(get_required_base_features=True, get_required_base_target=True, get_all_ta_cofig_output_columns=False, get_extra_direction_fields=False)]\n",
    "    # return [X,Y]\n",
    "\n",
    "    # Get current level index id\n",
    "    current_level_index = self.search_time_index(data_in_required_format, idx_start_date)\n",
    "    \n",
    "    # Get all input time step \n",
    "    all_inputs = self.get_multi_slice_backward_output(data_in_required_format, current_level_index)\n",
    "    hold_col = self.return_all_output_col(get_required_base_features=True, get_required_base_target=True, get_all_ta_cofig_output_columns=add_ta, get_extra_direction_fields=False)\n",
    "    all_inputs = [all_input[hold_col] for all_input in all_inputs]\n",
    "\n",
    "    if self.squeeze_forward:\n",
    "    #   # Get output\n",
    "      output_data = self.get_single_slice_forward_output(data_in_required_format, current_level_index)\n",
    "      if not cal_pct_chg: # Add trend\n",
    "        output_data.loc[output_data.index[0],self.get_extra_direction_fields()] = self.direction_close(all_inputs[-1].copy().tail(1).close.to_numpy()[0], output_data.copy().close.to_numpy()[0])\n",
    "      output_data = output_data[self.return_all_output_col(get_required_base_features=True, get_required_base_target=True, get_all_ta_cofig_output_columns=add_ta, get_extra_direction_fields=True)]\n",
    "\n",
    "    else:\n",
    "      output_data = self.get_multi_slice_forward_output(data_in_required_format, current_level_index)\n",
    "      if not cal_pct_chg:\n",
    "        import functools\n",
    "        def direction_close(close_next, close_prev=None):\n",
    "          return self.direction_close(close_prev=close_prev, close_next=close_next)\n",
    "\n",
    "        # Create a partial function with a default threshold value\n",
    "        direction_close = functools.partial(direction_close, close_prev=all_inputs[-1]['close'].iloc[-1])\n",
    "        \n",
    "        # Apply the comparison function to column A of the dataframe and create a new column\n",
    "        output_data = output_data.assign(\n",
    "            **{name: output_data['close'].apply(direction_close).str[i] for i, name in enumerate(self.get_extra_direction_fields())}\n",
    "        )    \n",
    "        output_data = output_data[self.return_all_output_col(get_required_base_features=True, get_required_base_target=True, get_all_ta_cofig_output_columns=add_ta, get_extra_direction_fields=True)]\n",
    "\n",
    "\n",
    "    if to_numpy:\n",
    "      all_inputs = np.array(all_inputs)\n",
    "      output_data = np.array(output_data)\n",
    "    return [all_inputs,output_data]\n",
    "\n",
    "  def slice_data_get(self, idxs, process_data=True,add_ta=True, apply_nat_log=False, fill_na=True, cal_pct_chg=False, to_numpy=True):\n",
    "    background_tasks = set()\n",
    "    results = []\n",
    "    for idx in idxs:\n",
    "      if not self.preload_data_file:\n",
    "        time.sleep(3)\n",
    "      results.append(self.async_get_data_by_idx(idx=idx, process_data=process_data,add_ta=add_ta, apply_nat_log=apply_nat_log, fill_na=fill_na, cal_pct_chg=cal_pct_chg,to_numpy=to_numpy))\n",
    "      if not self.preload_data_file:\n",
    "        time.sleep(3)\n",
    "    return results\n",
    "\n",
    "  def __getitem__(self,idxs):\n",
    "    return self.get_item(idxs)\n",
    "\n",
    "  def get_item(self, idxs, process_data=True,add_ta=True, apply_nat_log=False, fill_na=True, cal_pct_chg=False, to_numpy=True):\n",
    "    \"\"\"\n",
    "      Assuming all data points starting from the start date to the end date were \n",
    "      available, this method selects the nth row from the full data set.\n",
    "    \"\"\"\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "    # print(f\"Checking slice: {idxs}\")\n",
    "    if isinstance(idxs, list):  \n",
    "      pass\n",
    "    elif isinstance(idxs, slice):  \n",
    "      start, stop, step = idxs.start or 0, idxs.stop or len(self), idxs.step or 1\n",
    "      # print(\"start, stop, step\",start, stop, step)\n",
    "      idxs = range(start, stop, step)\n",
    "    elif isinstance(idxs, int):  \n",
    "      idxs = [idxs]\n",
    "    else:\n",
    "      raise ValueError(\"Type slice of int required\")\n",
    "\n",
    "    # Get data\n",
    "    try:\n",
    "      # loop = asyncio.get_event_loop()\n",
    "      # background_tasks = self.slice_data_get(idxs)\n",
    "      # background_tasks = loop.run_until_complete(background_tasks)\n",
    "      # background_tasks = asyncio.run(self.slice_data_get(idxs))\n",
    "      background_tasks = self.slice_data_get(idxs, process_data=process_data,add_ta=add_ta,apply_nat_log=apply_nat_log, fill_na=fill_na, cal_pct_chg=cal_pct_chg, to_numpy=to_numpy)\n",
    "      # loop.close()\n",
    "    except Exception as e:\n",
    "      raise Exception(f\"Exception {e} occurred while getting result from self.slice_data_get\")\n",
    "    \n",
    "    # Extract data\n",
    "    X,Y = [],[]\n",
    "    for each_task in background_tasks:\n",
    "      X.append(each_task[0])\n",
    "      Y.append(each_task[1])\n",
    "      # Y2.append(each_task[2])\n",
    "\n",
    "    # Expand data dim\n",
    "    if to_numpy:\n",
    "      try:\n",
    "        X = np.stack(X, axis=0)\n",
    "        # X = X.squeeze(1)\n",
    "\n",
    "        Y = np.stack(Y, axis=0)\n",
    "        # Y = Y.squeeze(1) \n",
    "\n",
    "      except Exception as e:\n",
    "        raise Exception(f\"Exception {e} occurred while expanding dimension of output X of shape {X.shape} and Y of shape {Y.shape}\")\n",
    "\n",
    "    # for each in background_tasks: print(each)\n",
    "    return [X,Y]\n",
    "\n",
    "  \n",
    "\n",
    "  def get_real_start_timeframe(self, start=None):\n",
    "    global_start_time = self.start if not start else start\n",
    "    walk_forward_time = self.start + pd.Timedelta(minutes=1500)\n",
    "    base_data = self.qb.History(self.symbol, global_start_time, walk_forward_time).loc[self.symbol].iloc[[0]].index\n",
    "    return base_data\n",
    "  \n",
    "  def get_real_end_timeframe(self, end=None):\n",
    "    global_end_time = self.end if not end else end\n",
    "    walk_backward_time = global_end_time - pd.Timedelta(minutes=5000)\n",
    "    base_data = self.qb.History(self.symbol, walk_backward_time, global_end_time).loc[self.symbol].iloc[[-1]].index\n",
    "    return base_data\n",
    "\n",
    "  def replace_inf(self, X):\n",
    "      # Replace inf with max value for each feature column\n",
    "      X = X.clip(lower=X.min(), upper=X.max(), axis=1)\n",
    "      return X\n",
    "\n",
    "  def cal_pct_chg(self,X,fill_na):\n",
    "    # Fill na with zeros\n",
    "    if fill_na: X = X.pct_change().fillna(0)\n",
    "    return self.replace_inf(X)\n",
    "\n",
    "  def direction_close(self,close_prev,close_next):\n",
    "    if close_prev < close_next:\n",
    "      return [0,0,1]\n",
    "    elif close_prev > close_next:\n",
    "      return [1,0,0]\n",
    "    else:\n",
    "      return [0,1,0]\n",
    "\n",
    "  def direction_from_pct_chg(self,x):\n",
    "    if x < 0:\n",
    "      return [1,0,0]\n",
    "    elif x > 0:\n",
    "      return [0,0,1]\n",
    "    else:\n",
    "      return [0,1,0]\n",
    "\n",
    "  def apply_nat_log(self, X):\n",
    "    X = X.apply(lambda x:np.log(x))\n",
    "    X = self.replace_inf(X)\n",
    "    return X\n",
    "\n",
    "  def process_data_raw(self, data_in_required_format, add_ta=True, apply_nat_log=False, fill_na=True, cal_pct_chg=False):\n",
    "    # print(\"In process_data_raw\")\n",
    "    if add_ta:\n",
    "      # Add technical analysis\n",
    "      data_in_required_format = self.add_technical_analysis_to_dataset(data_in_required_format)\n",
    "    # print(\"after ta\")\n",
    "    #Apply natural log to all numerical data features\n",
    "    if apply_nat_log:\n",
    "      data_in_required_format = self.apply_nat_log(data_in_required_format)\n",
    "    # print(\"after log\")\n",
    "\n",
    "    #Convert to pct_change\n",
    "    if cal_pct_chg:\n",
    "      # Replace inf with max value for each feature column\n",
    "      # print(\"Before cal_pct_chg\")\n",
    "      data_in_required_format = self.cal_pct_chg(data_in_required_format,fill_na=fill_na)\n",
    "      data_in_required_format.loc[data_in_required_format.index, self.get_extra_direction_fields()] =  data_in_required_format[\"close\"].apply(lambda x: self.direction_from_pct_chg(x)).to_list()\n",
    "      # print(\"After cal_pct_chang\")\n",
    "      # print(data_in_required_format['direction'])\n",
    "      # if 'direction' not in self._extra_feature_target_fields:\n",
    "      #   self._extra_feature_target_fields.append('direction')\n",
    "    return data_in_required_format[self.return_all_output_col(get_required_base_features=True, get_required_base_target=True, get_all_ta_cofig_output_columns=add_ta, get_extra_direction_fields=cal_pct_chg)]\n",
    "\n",
    "  def scale_raw_data(self, X):\n",
    "    \"\"\"\n",
    "      Method used to scale data\n",
    "    \"\"\"\n",
    "    assert self.data_scaled, \"Data has not been transformed, please run self.partially_scale_data()\"\n",
    "    x_shape = X.shape\n",
    "    scaled_data = self.scaler.transform(X.reshape(-1, X.shape[-1]))\n",
    "    return scaled_data.reshape(x_shape)\n",
    "\n",
    "  def partially_scale_data(self):\n",
    "    \"\"\"\n",
    "      Due to the volumn of the data if we were to scale in sec and minute, this \n",
    "      method is used to loop through the data in chunks and scale from train \n",
    "      start_date to train end_date.\n",
    "    \"\"\"\n",
    "    self.data_scaled = False\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    step = 10\n",
    "    for idx in range(step,len(self), step):\n",
    "      print(idx-step,idx)\n",
    "      X = self[idx-step:idx][0]\n",
    "      scaler.partial_fit(X.reshape(-1, X.shape[-1]))\n",
    "    self.scaler = scaler\n",
    "    self.data_scaled = True\n",
    "    return scaler\n",
    "\n",
    "  def backend_get_all_raw(self, asset=None, resolution=None, start_date=None, end_date=None, **kwarg):\n",
    "    # print(\"ya\",start_date,end_date)\n",
    "    (asset, resolution, start_date, end_date) = (asset if asset else self.asset, resolution if resolution else self.resolution,\\\n",
    "    start_date if start_date else self.start_date,end_date if end_date else self.end_date)\n",
    "    # print(\"ya\",start_date,end_date)\n",
    "    start_date = self.convert_date_to_backend_format(start_date, format='%Y-%m-%d-%H-%M')\n",
    "    end_date = self.convert_date_to_backend_format(end_date, format='%Y-%m-%d-%H-%M'  )\n",
    "\n",
    "    # print((asset, resolution, start_date, end_date))\n",
    "    # print(f\"self._preload_data: {self._preload_data}\")\n",
    "    # print(\"before slice\")\n",
    "    if self._preload_data:\n",
    "      res = self.load_from_cache(asset, resolution, start_date, end_date, **kwarg)\n",
    "    else:\n",
    "      res = self.load_from_api(asset, resolution, start_date, end_date, **kwarg)\n",
    "    # print(f\"Data --> {res.shape}\")\n",
    "    # print(\"After slice\")\n",
    "    return res\n",
    "\n",
    "  def init_preload(self,**kwarg):\n",
    "    \"\"\"\n",
    "      This method calls the preload_data_api\n",
    "      method to preload the base data for a specific \n",
    "      data range, this data is what is expanded at each \n",
    "      call, the advantage of implementing a preload method\n",
    "      is to overcome the time take between data request and\n",
    "      when the api endpoint returns a result. With this method\n",
    "      correctly implented, the data class just now needs to expand\n",
    "      the base data at each datapoint request. \n",
    "    \"\"\"\n",
    "    # print(f\"Testing init_preload method: kwarg.get('preload_data_api', False): {kwarg.get('preload_data_api', False)}, kwarg.get('preload_data_file', False): {kwarg.get('preload_data_file', False)}\")\n",
    "    self._preload_data = False\n",
    "    if kwarg.get('preload_data_api', False):\n",
    "      self.preload_data_api(**kwarg)\n",
    "    elif kwarg.get('preload_data_file', False):\n",
    "      self.preload_data_file(**kwarg)\n",
    "\n",
    "  def get_preload_safe_start_end_date(self):\n",
    "    _, idx_backwards_date, _ = self.get_idx_start_back_end_date(0)\n",
    "    _, _, idx_end_date = self.get_idx_start_back_end_date(len(self))\n",
    "    return idx_backwards_date, idx_end_date\n",
    "    \n",
    "  def preload_data_api(self, **kwarg):\n",
    "    # _, idx_backwards_date, _ = self.get_idx_start_back_end_date(0)\n",
    "    # _, _, idx_end_date = self.get_idx_start_back_end_date(len(self))\n",
    "    idx_backwards_date, idx_end_date = self.get_preload_safe_start_end_date()\n",
    "\n",
    "    self.cache = self.backend_get_all_raw(asset=self.asset, resolution=self.resolution, start_date=idx_backwards_date, end_date=idx_end_date, **kwarg)\n",
    "    self._preload_data = True   \n",
    "    self.presave_data()\n",
    "\n",
    "  def default_data_path(self):\n",
    "    return f\"inputs/{self.get_backend()}_asset={self.asset},resolution={self.resolution},start_date={self.start_date}, end_date={self.end_date}\"\n",
    "\n",
    "  def presave_data(self):\n",
    "    if self._preload_data:\n",
    "      self.cache.to_csv(self.default_data_path())\n",
    "    else:\n",
    "      raise NotImplementedError(\"Preload not available, preload method should set self._preload_data to true once load is complete\")\n",
    "\n",
    "  def preload_data_file(self, **kwarg):\n",
    "    if kwarg.get('preload_data_file'):\n",
    "      path = kwarg.get('data_file', None) \n",
    "      if path == None:\n",
    "        path = self.default_data_path()\n",
    "        print(f\"Warning, preload is using the dafault data path {path}, inspect if this is not the desired performance\")\n",
    "      try:\n",
    "        self.cache = pd.read_csv(path)\n",
    "        self.cache[\"time\"] = pd.to_datetime(self.cache[\"time\"])\n",
    "        self.cache.set_index(\"time\", drop=True, inplace=True)\n",
    "        self._preload_data = True\n",
    "      except Exception as e:\n",
    "        raise Exception(f\"Error {e} occured while trying to load data from file {kwarg.get('data_file', None)}\")\n",
    "      self.presave_data()\n",
    "\n",
    "    #filter result to increase memory size\n",
    "    idx_backwards_date, idx_end_date = self.get_preload_safe_start_end_date()\n",
    "    idx_backwards_date, idx_end_date = self.convert_date_to_backend_format(idx_backwards_date, format='%Y-%m-%d-%H-%M'), self.convert_date_to_backend_format(idx_end_date, format='%Y-%m-%d-%H-%M')\n",
    "    self.cache = self.load_from_cache(asset=self.asset, resolution=self.resolution, start_date=idx_backwards_date, end_date=idx_end_date)\n",
    "\n",
    "\n",
    "  def load_from_cache(self, asset, resolution, start_date, end_date,**kwarg):\n",
    "    # print(f\"self.cache: {self.cache}\")\n",
    "    # convert_date_from_backend_format\n",
    "    start_date = self.convert_date_from_backend_format(start_date, format='%Y-%m-%d-%H-%M')\n",
    "    end_date = self.convert_date_from_backend_format(end_date, format='%Y-%m-%d-%H-%M')\n",
    "    try:\n",
    "      res = self.cache[(self.cache.index >= start_date) & (self.cache.index <= end_date)].copy()\n",
    "      return res\n",
    "    except Exception as e:\n",
    "      raise Exception(f\"Error {e} occured while getting data from cache\")\n",
    "\n",
    "\n",
    "  def load_from_api(self, asset, resolution, start_date, end_date,**kwarg):\n",
    "    \"\"\"\n",
    "      Implement this method  in the  sub class, to receive data from a specified\n",
    "      start and end datetime.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Sub-class Should implement this method\")\n",
    "\n",
    "class QuantConnectDataBackend(DatasetBaseBackend):\n",
    "  def __init__(self):\n",
    "    raise NotImplementedError(\"Sub-class Should implement this method\")\n",
    "\n",
    "class FXCMDataBackend(DatasetBaseBackend):\n",
    "  def __init__(self):\n",
    "    raise NotImplementedError(\"Sub-class Should implement this method\")\n",
    "\n",
    "class yFinanceDataBackend(DatasetBaseBackend):\n",
    "  def __init__(self):\n",
    "    raise NotImplementedError(\"Sub-class Should implement this method\")\n",
    "\n",
    "class cryptocmdDataBackend(DatasetBaseBackend):\n",
    "  def __init__(self):\n",
    "    raise NotImplementedError(\"Sub-class Should implement this method\")\n",
    "\n",
    "class historicCryptoBackend(DatasetBaseBackend):\n",
    "  def __init__(self,**kwarg):\n",
    "    self.set_backend(\"historicCryptoBackend\")\n",
    "    super().__init__(**kwarg)\n",
    "\n",
    "  def load_from_api(self, asset, resolution, start_date, end_date,**kwarg):\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "    raw_data = asyncio.run(HistoricalData(asset, resolution, start_date, end_date,verbose = kwarg.get('verbose', False)).retrieve_data())\n",
    "    return raw_data\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Section - Model(Using Transformers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def wasiu_input_check(class_level):\n",
    "    \"\"\"\n",
    "        param: classtype: this is the expected classes to check if input aligns \n",
    "        with format, Values: [\"wasiunet\", \"wasiuspace\",\"wasiuspacetime\"]\n",
    "    \"\"\"\n",
    "    def checker_decorator(func):\n",
    "        \n",
    "        @functools.wraps(func)\n",
    "        def checker_function(*args, **kwargs):\n",
    "          src = kwargs.get('src',None)\n",
    "          trg = kwargs.get('trg',None)\n",
    "\n",
    "          if type(src) == type(None):\n",
    "            src = args[1]\n",
    "\n",
    "          # Check input type aligns with required input\n",
    "          if class_level in [\"wasiunet\",\"wasiuspacetime\"]:\n",
    "            if type(trg) == type(None):\n",
    "              trg = args[2]\n",
    "            assert (\n",
    "                (torch.is_tensor(src) or isinstance(src, np.ndarray)) \n",
    "                and \n",
    "                (torch.is_tensor(trg) or isinstance(trg, np.ndarray)) \n",
    "              ), f\"src input of type {type(src)} or trg input of type {type(trg)} is not tensor or numpy, supported type for input is Pytorch tensor or numpy array.\"\n",
    "\n",
    "          else:\n",
    "            assert torch.is_tensor(src) or isinstance(src, np.ndarray), f\"src input of type {type(src)} supported type for input is Pytorch tensor or numpy array.\"\n",
    "\n",
    "          # Convert src and trg to tensor\n",
    "          # src shape --> batch_size * time_seq_len * space_seq_len * feat_len e.g (2 * 12 * 60 * 7) or \n",
    "          # time_seq_len * space_seq_len * feat_len e.g (12 * 60 * 7)\n",
    "          src = torch.Tensor(src)\n",
    "\n",
    "          if class_level in [\"wasiunet\",\"wasiuspacetime\"]:\n",
    "            # trg shape -->  space_seq_len * batch_size * feat_len e.g (2 * 100 * 7)\n",
    "            trg = torch.Tensor(trg)\n",
    "          \n",
    "          if class_level in [\"wasiunet\"]:\n",
    "            # Write assertion to confirm input shapes\n",
    "            if src.ndim == 4:\n",
    "              pass\n",
    "            elif src.ndim == 3:\n",
    "              src = src.unsqueeze(0) # expand dimension\n",
    "            else:\n",
    "              raise ValueError(f\"You have a dimension error in your source input of {src.ndim}, expected dimension is {4} or {3}\")\n",
    "\n",
    "            # Write assertion to confirm output shapes\n",
    "            if trg.ndim == 3:\n",
    "              pass\n",
    "            elif trg.ndim == 2:\n",
    "              trg = trg.unsqueeze(0) # expand dimension\n",
    "            else:\n",
    "              raise ValueError(f\"You have a dimension error in your source input of {src.ndim}, expected dimension is {4} or {3}\")\n",
    "\n",
    "          # assert src and trg have the same batch size\n",
    "          if class_level in [\"wasiunet\",\"wasiuspacetime\"]:\n",
    "            assert src.shape[0] == trg.shape[0], f\"source batch {src.shape[1]} and target batch {trg.shape[0]} do not have the same batch size\"\n",
    "            return func(src=src, trg=trg)\n",
    "          else:\n",
    "            return func(src=src)\n",
    "        \n",
    "        return checker_function\n",
    "    return checker_decorator\n",
    "\n",
    "class FCView(nn.Module):\n",
    "    def __init__(self,shape=None):\n",
    "        super(FCView, self).__init__()\n",
    "        if shape != None:\n",
    "            self.shape = shape \n",
    "        else:\n",
    "            self.shape = -1\n",
    "\n",
    "    # noinspection PyMethodMayBeStatic\n",
    "    def forward(self, x):\n",
    "        n_b = x.data.size(0)\n",
    "        x = x.view(self.shape) if self.shape != -1 else x.view(n_b, -1)\n",
    "        return x\n",
    "\n",
    "class WasiuSpace(nn.Module):\n",
    "      \"\"\"\n",
    "        This class provides the functionality to process assets at a space level, \n",
    "        by space level this means for example we are processing a 60 sequences of \n",
    "        1 min timeframe asset data or processing a 60 sequences of 5mins timeframe\n",
    "        asset data. So basically we embed positions on space level and feed to \n",
    "        the transformer model.\n",
    "        \n",
    "        :param: d_model: a singular Cryptocurrency ticker. (str)\n",
    "        :param: src_vocab_size: the price data frequency in seconds, one of: 60, 300, 900, 3600, 21600, 86400. (int)\n",
    "        :param: trg_vocab_size: a date string in the format YYYY-MM-DD-HH-MM. (str)\n",
    "        :param: num_heads: printing during extraction, Default=True. (bool)\n",
    "        :param: num_encoder_layers: a Pandas DataFrame which contains requested cryptocurrency data. (pd.DataFrame)\n",
    "        :param: forward_expansion: a date string in the format YYYY-MM-DD-HH-MM,  Default=Now. (str)\n",
    "        :param: dropout: printing during extraction, Default=True. (bool)\n",
    "        :param: max_len: a Pandas DataFrame which contains requested cryptocurrency data. (pd.DataFrame)\n",
    "        :param: device: a Pandas DataFrame which contains requested cryptocurrency data. (pd.DataFrame)\n",
    "      \"\"\"\n",
    "      def __init__(self, d_model, nhead, num_encoder_layers, dim_feedforward, \n",
    "                dropout, max_len, device,trans_activation,\n",
    "                 trans_norm_first,trans_batch_first\n",
    "                ):\n",
    "        super(WasiuSpace, self).__init__()\n",
    "        self.device = device\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # space model\n",
    "        self.wasiuspace_encoder_layer = nn.TransformerEncoderLayer(\n",
    "                                                        d_model=d_model, nhead=nhead, \n",
    "                                                        dim_feedforward=dim_feedforward, \n",
    "                                                        dropout=dropout,activation=trans_activation,\n",
    "                                                        batch_first=trans_batch_first,\n",
    "                                                        norm_first=trans_norm_first\n",
    "                                                        )\n",
    "        \n",
    "        self.wasiuspace_transformer_encoder = nn.TransformerEncoder(\n",
    "                                                          encoder_layer=self.wasiuspace_encoder_layer, \n",
    "                                                        num_layers=num_encoder_layers\n",
    "                                                        )\n",
    "      @wasiu_input_check('wasiuspace')\n",
    "      def wasiuspace_input_check(src):\n",
    "          return src\n",
    "\n",
    "      def forward(self, src):\n",
    "        # src shape: (src_seq_length, batch_size, time_seq_length, embed_size)\n",
    "        # trg shape: (trg_seq_length, batch_size)\n",
    "        src = self.wasiuspace_input_check(src)\n",
    "        return self.wasiuspace_transformer_encoder(src)\n",
    "\n",
    "class WasiuSpaceTime(nn.Module):\n",
    "      \"\"\"\n",
    "        This class provides the functionality to process assets at a time level, \n",
    "        by time level this means for example we are processing a 60 sequences of \n",
    "        1 min timeframe asset data on one end and processing a 60 sequences of 5mins timeframe\n",
    "        asset data on the other end to both return the next lowest timeframe series into\n",
    "        the future. So basically we embed positions to time level and we feed it to \n",
    "        the space transformer model.\n",
    "      \"\"\"\n",
    "\n",
    "      def __init__(self, embedding_dim,inp_feat, out_feat, in_channels, patch_size,space_seq_len, \n",
    "                   expand_HW, nhead, num_encoder_layers, num_decoder_layers, \n",
    "                  dim_feedforward, dropout, max_len, device,trans_activation,\n",
    "                 trans_norm_first,trans_batch_first\n",
    "                  ):\n",
    "        super(WasiuSpaceTime, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.max_len = max_len\n",
    "                                                         \n",
    "        # Encode positions\n",
    "        self.src_word_embedding = nn.Sequential(OrderedDict([\n",
    "                                  ('src_inp_norm_layer_1', nn.LayerNorm(inp_feat)), # Source Create the normalization layer\n",
    "                                  ('expand_HW',  nn.Flatten(start_dim=-2, end_dim=-1)),\n",
    "                                  ('src_lin_embedding', nn.Linear(space_seq_len*inp_feat, expand_HW*expand_HW)), # Source Create the word embedding layer\n",
    "                                  ('reshape_CHW', FCView(shape=(-1,in_channels, expand_HW, expand_HW))),\n",
    "                                  ('src_patcher_layer', nn.Conv2d(\n",
    "                                          in_channels = in_channels,\n",
    "                                          out_channels = embedding_dim,\n",
    "                                          kernel_size=patch_size,\n",
    "                                          stride = patch_size,\n",
    "                                          padding = 0\n",
    "                                          )), # Input patcher\n",
    "                                  ('flatten', nn.Flatten(start_dim=2, end_dim=3)),\n",
    "                                ]))\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        # Source Create the word position embedding layer\n",
    "        self.src_position_embeddding = nn.Embedding(max_len, embedding_dim)\n",
    "\n",
    "        # Target Create the word embedding layer\n",
    "        self.trg_word_embedding = nn.Sequential(OrderedDict([\n",
    "                                  ('trg_lin_embedding', nn.Linear(out_feat, embedding_dim)),\n",
    "                                  ('dropout', nn.Dropout(dropout))\n",
    "                                ]))\n",
    "        \n",
    "\n",
    "        # Target Create the word position embedding layer\n",
    "        self.trg_position_embedding = nn.Embedding(max_len, embedding_dim)\n",
    "\n",
    "        # space-time model\n",
    "        self.wasiuspace_encoder = WasiuSpace(d_model=embedding_dim, nhead=nhead, \n",
    "                                            num_encoder_layers=num_encoder_layers, \n",
    "                                            dim_feedforward=dim_feedforward,\n",
    "                                            dropout=dropout, max_len=max_len, \n",
    "                                            device=device,trans_activation=trans_activation,\n",
    "                                                        trans_batch_first=trans_batch_first,\n",
    "                                                        trans_norm_first=trans_norm_first\n",
    "                                          )\n",
    "        \n",
    "        # space-time model\n",
    "        self.wasiuspacetime_decoder_layer = nn.TransformerDecoderLayer(\n",
    "                                                        d_model=embedding_dim, nhead=nhead, \n",
    "                                                        dim_feedforward=dim_feedforward, \n",
    "                                                        dropout=dropout,activation=trans_activation,\n",
    "                                                        batch_first=trans_batch_first,\n",
    "                                                        norm_first=trans_norm_first\n",
    "                                                        )\n",
    "        \n",
    "        self.wasiuspacetime_transformer_decoder = nn.TransformerDecoder(\n",
    "                                                          decoder_layer=self.wasiuspacetime_decoder_layer, \n",
    "                                                       num_layers=num_decoder_layers)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "      @wasiu_input_check('wasiuspacetime')\n",
    "      def wasiuspacetime_input_check(src, trg):\n",
    "          return src, trg\n",
    "\n",
    "      def forward(self, src, trg):\n",
    "        \"\"\"\n",
    "          :param: src: source asset data in the format \n",
    "            time_seq_len * batch_size * space_seq_len * feat_len \n",
    "            e.g (2 * 12 * 60 * 7) or (1 * 12 * 60 * 7).\n",
    "            (Tensor)\n",
    "\n",
    "          :param: trg: target asset data in the format \n",
    "            space_seq_len * batch_size * feat_len e.g (100 * 2 * 7) or (100 * 1 * 7)\n",
    "            (Tensor)\n",
    "        \"\"\"\n",
    "        src, trg =  self.wasiuspacetime_input_check(src, trg)\n",
    "        src = self.src_word_embedding(src).permute(0,2,1) # embed and reshape\n",
    "        \n",
    "        # Get src and trg shapes\n",
    "        batch_size, src_seq_length, _ = src.shape\n",
    "        _, trg_seq_length, _ = trg.shape\n",
    "        \n",
    "        # Create Positions\n",
    "        src_position = (\n",
    "            torch.arange(0, src_seq_length).unsqueeze(0).expand(batch_size, src_seq_length)\n",
    "            .to(self.device)\n",
    "        )\n",
    "\n",
    "        trg_position = (\n",
    "            torch.arange(0, trg_seq_length).unsqueeze(0).expand(batch_size, trg_seq_length)\n",
    "            .to(self.device)\n",
    "        )\n",
    "\n",
    "        # Embed positions into data\n",
    "        embed_src = self.dropout(\n",
    "            (src + self.src_position_embeddding(src_position))\n",
    "        )\n",
    "\n",
    "        embed_trg = self.dropout(\n",
    "            (self.trg_word_embedding(trg) + self.trg_position_embedding(trg_position)) \n",
    "        )\n",
    "        trg_mask = nn.Transformer.generate_square_subsequent_mask(trg_seq_length).to(\n",
    "            self.device\n",
    "        )\n",
    "\n",
    "        encoded_src_memory = self.wasiuspace_encoder(embed_src)\n",
    "\n",
    "        out = self.wasiuspacetime_transformer_decoder(embed_trg,encoded_src_memory,tgt_mask=trg_mask)\n",
    "        return out\n",
    "\n",
    "class WasiuNet(pl.LightningModule):\n",
    "    def __init__(self, embedding_dim, inp_feat, out_feat, in_channels, patch_size, space_seq_len,\n",
    "                 expand_HW, nhead, num_encoder_layers, num_decoder_layers, \n",
    "                 dim_feedforward, dropout, max_len, device,trans_activation,\n",
    "                 trans_norm_first,trans_batch_first,feat_map_dict\n",
    "                  ):\n",
    "        super(WasiuNet, self).__init__()\n",
    "        # super(Transformer, self).__init__()\n",
    "        # self.src_word_embedding = nn.Embedding(src_vocab_size, embedding_size)\n",
    "        # self.src_position_embeddding = nn.Embedding(max_len, embedding_size)\n",
    "        # self.trg_word_embedding = nn.Embedding(trg_vocab_size, embedding_size)\n",
    "        # self.trg_position_embedding = nn.Embedding(max_len, embedding_size)\n",
    "        self.feat_map_dict = feat_map_dict\n",
    "        self.target_feature = []\n",
    "        self.direction_feature = []\n",
    "        for key, value in self.feat_map_dict.items():\n",
    "          if key in [ 'direction_-1_conf','direction_0_conf','direction_1_conf']:\n",
    "            self.direction_feature.append(value)\n",
    "          else:\n",
    "            self.target_feature.append(value)\n",
    "\n",
    "        self.wasiuspacetime = WasiuSpaceTime(embedding_dim=embedding_dim, inp_feat=inp_feat, \n",
    "                                             out_feat=out_feat, in_channels=in_channels, \n",
    "                                             patch_size=patch_size, space_seq_len=space_seq_len, expand_HW=expand_HW,\n",
    "                                             nhead=nhead, num_encoder_layers=num_encoder_layers, \n",
    "                                              num_decoder_layers=num_decoder_layers,\n",
    "                                              dim_feedforward=dim_feedforward,\n",
    "                                              dropout=dropout, max_len=max_len, \n",
    "                                              device=device,trans_activation=trans_activation,\n",
    "                                              trans_batch_first=trans_batch_first,\n",
    "                                              trans_norm_first=trans_norm_first\n",
    "                                          )\n",
    "        # Target Create the word embedding layer\n",
    "        self.fc_out = nn.Sequential(OrderedDict([\n",
    "                                  ('dropout', nn.Dropout(dropout)),  \n",
    "                                  ('decoded_out_1', nn.Linear(embedding_dim, out_feat)),\n",
    "                                  ('relu',nn.ReLU()),\n",
    "                                  ('decoded_out_2', nn.Linear(out_feat, out_feat))\n",
    "                                  \n",
    "                                ]))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @wasiu_input_check('wasiunet')\n",
    "    def check_wasiunet_input(src, trg):\n",
    "        return src, trg\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.1)\n",
    "        return optimizer\n",
    "\n",
    "    def criterion(self, multi_label_pred, multi_label_target, regression_pred, regression_target):\n",
    "\n",
    "        # Define the multi-label loss function\n",
    "        multi_label_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Define the regression loss function\n",
    "        regression_loss_fn = nn.MSELoss()\n",
    "\n",
    "        # Define a weight for the multi-label loss (you can adjust this to balance the two losses)\n",
    "        multi_label_loss_weight = 0.5\n",
    "\n",
    "        # Define a weight for the regression loss (you can adjust this to balance the two losses)\n",
    "        regression_loss_weight = 0.5\n",
    "\n",
    "        # Compute the multi-label loss\n",
    "        multi_label_loss = multi_label_loss_fn(multi_label_pred.float(), multi_label_target.float())\n",
    "        \n",
    "        # Compute the regression loss\n",
    "        regression_loss = regression_loss_fn(regression_pred.float(), regression_target.float())\n",
    "        \n",
    "        # Combine the two losses with the specified weights\n",
    "        combined_loss = multi_label_loss_weight * multi_label_loss + regression_loss_weight * regression_loss\n",
    "        return combined_loss\n",
    "\n",
    "    def forward(self, src, trg):        \n",
    "        \"\"\"\n",
    "          :param: src: source asset data in the format \n",
    "            time_seq_len * batch_size * space_seq_len * feat_len \n",
    "            e.g (2 * 12 * 60 * 7).\n",
    "            or time_seq_len * space_seq_len * feat_len e.g (60 * 12 * 7)\n",
    "            (Numpy | Tensor)\n",
    "\n",
    "          :param: trg: target asset data in the format \n",
    "            space_seq_len * batch_size * feat_len e.g (100 * 2 * 7)\n",
    "            or time_seq_len * feat_len e.g (100 * 7)\n",
    "            (Numpy | Tensor)\n",
    "        \"\"\"\n",
    "        src, trg = self.check_wasiunet_input(src, trg)\n",
    "        src, trg = src.type(torch.float32).to(self.device), trg.type(torch.float32).to(self.device)\n",
    "        out = self.wasiuspacetime(src, trg)\n",
    "        del src, trg\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        x, y = batch\n",
    "        x = batch[0].squeeze(1)\n",
    "        y = batch[1].squeeze(1)\n",
    "\n",
    "        output = self(x, y[:,:-1,:])\n",
    "        y = y[:,1:,:]\n",
    "        \n",
    "        # multi_label_pred, multi_label_target, regression_pred, regression_target\n",
    "        # Compute the multi-label loss\n",
    "        loss = self.criterion(output[:,:,self.direction_feature], y[:,:,self.direction_feature], output[:,:,self.target_feature], y[:,:,self.target_feature])\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return dict(\n",
    "            loss=loss,\n",
    "            log=dict(\n",
    "                train_loss=loss\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # this is the test loop\n",
    "        x, y = batch\n",
    "        x = batch[0].squeeze(1)\n",
    "        y = batch[1].squeeze(1)\n",
    "\n",
    "        output = self(x, y[:,:-1,:])\n",
    "        y = y[:,1:,:]\n",
    "        \n",
    "        # multi_label_pred, multi_label_target, regression_pred, regression_target\n",
    "        # Compute the multi-label loss\n",
    "        loss = self.criterion(output[:,:,self.direction_feature], y[:,:,self.direction_feature], output[:,:,self.target_feature], y[:,:,self.target_feature])\n",
    "        self.log(\"test_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return dict(\n",
    "            loss=loss,\n",
    "            log=dict(\n",
    "                train_loss=loss\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # this is the validation loop\n",
    "        x, y = batch\n",
    "        x = batch[0].squeeze(1)\n",
    "        y = batch[1].squeeze(1)\n",
    "\n",
    "        output = self(x, y[:,:-1,:])\n",
    "        y = y[:,1:,:]\n",
    "        \n",
    "        # multi_label_pred, multi_label_target, regression_pred, regression_target\n",
    "        # Compute the multi-label loss\n",
    "        loss = self.criterion(output[:,:,self.direction_feature], y[:,:,self.direction_feature], output[:,:,self.target_feature], y[:,:,self.target_feature])\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return dict(\n",
    "            loss=loss,\n",
    "            log=dict(\n",
    "                train_loss=loss\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        x = batch[0].squeeze(1)\n",
    "        y = batch[1].squeeze(1)\n",
    "\n",
    "        output = self(x, y[:,:-1,:])\n",
    "        return output\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup data and hyperparameters for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ta_config = [\n",
    "              # SMA - Simple Moving Average\n",
    "              {\"ta_func_name\":\"SMA\", 'ta_func_config':{'window':60,'fillna':False}},\n",
    "              {\"ta_func_name\":\"SMA\", 'ta_func_config':{'window':50,'fillna':False}},\n",
    "              {\"ta_func_name\":\"SMA\", 'ta_func_config':{'window':45,'fillna':False}},\n",
    "              {\"ta_func_name\":\"SMA\", 'ta_func_config':{'window':20,'fillna':False}},\n",
    "              {\"ta_func_name\":\"SMA\", 'ta_func_config':{'window':10,'fillna':False}},\n",
    "              {\"ta_func_name\":\"SMA\", 'ta_func_config':{'window':7,'fillna':False}},\n",
    "              {\"ta_func_name\":\"SMA\", 'ta_func_config':{'window':5,'fillna':False}},\n",
    "              {\"ta_func_name\":\"SMA\", 'ta_func_config':{'window':3,'fillna':False}},\n",
    "\n",
    "              # RSI\n",
    "              {\"ta_func_name\":\"RSI\", 'ta_func_config':{'window':60,'fillna':False}},\n",
    "              {\"ta_func_name\":\"RSI\", 'ta_func_config':{'window':50,'fillna':False}},\n",
    "              {\"ta_func_name\":\"RSI\", 'ta_func_config':{'window':45,'fillna':False}},\n",
    "              {\"ta_func_name\":\"RSI\", 'ta_func_config':{'window':20,'fillna':False}},\n",
    "              {\"ta_func_name\":\"RSI\", 'ta_func_config':{'window':10,'fillna':False}},\n",
    "              {\"ta_func_name\":\"RSI\", 'ta_func_config':{'window':7,'fillna':False}},\n",
    "              {\"ta_func_name\":\"RSI\", 'ta_func_config':{'window':5,'fillna':False}},\n",
    "              {\"ta_func_name\":\"RSI\", 'ta_func_config':{'window':3,'fillna':False}},\n",
    "\n",
    "              # STC\n",
    "              {\"ta_func_name\":\"STC\", 'ta_func_config':{'window_fast':60,'window_slow':5, 'fillna':False}},\n",
    "              {\"ta_func_name\":\"STC\", 'ta_func_config':{'window_fast':50,'window_slow':4, 'fillna':False}},\n",
    "              {\"ta_func_name\":\"STC\", 'ta_func_config':{'window_fast':45,'window_slow':3, 'fillna':False}},\n",
    "              {\"ta_func_name\":\"STC\", 'ta_func_config':{'window_fast':20,'window_slow':2, 'fillna':False}},\n",
    "              {\"ta_func_name\":\"STC\", 'ta_func_config':{'window_fast':10,'window_slow':5, 'fillna':False}},\n",
    "              {\"ta_func_name\":\"STC\", 'ta_func_config':{'window_fast':7,'window_slow':4, 'fillna':False}},\n",
    "              {\"ta_func_name\":\"STC\", 'ta_func_config':{'window_fast':5,'window_slow':3, 'fillna':False}},\n",
    "              {\"ta_func_name\":\"STC\", 'ta_func_config':{'window_fast':3,'window_slow':2, 'fillna':False}}\n",
    "             \n",
    "             ]\n",
    "# CCXT - Will use this for trading and analysis\n",
    "# Download data (this is just used to download the data and save it to current folder)\n",
    "# Data hyperparamters\n",
    "technical_analysis_config=ta_config\n",
    "asset = \"BTC-USD\"\n",
    "resolution = 60\n",
    "fea_output_per_data_slice = 60\n",
    "fea_data_slice = 12\n",
    "glob_time_step_forwards= 180\n",
    "batch_size = 32\n",
    "num_worker = 2\n",
    "start_date = '2016-01-14-00-00-00'\n",
    "end_date = '2023-01-08-00-00-00'\n",
    "file_path = \"inputs/historicCryptoBackend_asset=BTC-USD,resolution=60,start_date=2016-01-14 00:00:00, end_date=2023-01-08 00:00:00\"\n",
    "\n",
    "if file_path==None:\n",
    "  # download data\n",
    "  downloaded_data =  historicCryptoBackend(start_date=start_date, end_date=end_date, asset=asset, resolution=60,preload_data_api=True)\n",
    "  del downloaded_data\n",
    "\n",
    "btc_usd_train =  historicCryptoBackend(start_date='2022-01-01-00-00-00', end_date='2023-01-01-00-00-00', asset=asset, resolution=resolution,technical_analysis_config=ta_config,fea_output_per_data_slice=fea_output_per_data_slice, fea_data_slice=fea_data_slice,glob_time_step_forwards=glob_time_step_forwards, preload_data_api=False, preload_data_file=True, data_file=file_path, verbose=True)\n",
    "train_loader = DataLoader(btc_usd_train,batch_size=batch_size, shuffle=True, num_workers=num_worker)\n",
    "\n",
    "# val data engineering\n",
    "btc_usd_val =  historicCryptoBackend(start_date='2023-01-01-00-00-00', end_date='2023-01-05-00-00-00', asset=asset, resolution=resolution,technical_analysis_config=ta_config,fea_output_per_data_slice=fea_output_per_data_slice, fea_data_slice=fea_data_slice,glob_time_step_forwards=glob_time_step_forwards, preload_data_api=False, preload_data_file=True, data_file=file_path, verbose=True)\n",
    "valid_loader = DataLoader(btc_usd_val,batch_size=batch_size, shuffle=True, num_workers=num_worker)\n",
    "\n",
    "# test data engineering\n",
    "btc_usd_test =  historicCryptoBackend(start_date='2023-01-05-00-00-00', end_date='2023-01-07-00-00-00', asset=asset, resolution=resolution,technical_analysis_config=ta_config,fea_output_per_data_slice=fea_output_per_data_slice, fea_data_slice=fea_data_slice,glob_time_step_forwards=glob_time_step_forwards, preload_data_api=False, preload_data_file=True, data_file=file_path, verbose=True)\n",
    "test_loader = DataLoader(btc_usd_test,batch_size=batch_size, shuffle=True, num_workers=num_worker)\n",
    "\n",
    "print(f\"btc_usd_train:- {len(btc_usd_train)}, btc_usd_test:- {len(btc_usd_test)}, btc_usd_val:- {len(btc_usd_val)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = btc_usd_train.get_item([-1, 0], process_data = True, add_ta = True, apply_nat_log = False, fill_na = True, cal_pct_chg = False, to_numpy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the training phase\n",
    "device_name = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device_name)\n",
    "load_model = False\n",
    "save_model = True\n",
    "train_model = True\n",
    "\n",
    "# Training hyperparameters\n",
    "num_epochs = 5\n",
    "learning_rate = 3e-4\n",
    "# batch_size = 32\n",
    "\n",
    "# Model hyperparameters\n",
    "embedding_dim = 768 # This is the num of input features expected into encoder and decoder.\n",
    "patch_size = 16\n",
    "in_channels = X.shape[-3]\n",
    "space_seq_len = X.shape[-2]\n",
    "expand_HW = 224\n",
    "inp_feat = X.shape[-1]\n",
    "out_feat = Y.shape[-1]\n",
    "nhead = 4 #12\n",
    "num_encoder_layers = 2 #12 # This is the num of Encoder transformers\n",
    "num_decoder_layers = 2 #12 # This is the num of Decoder transformers\n",
    "dim_feedforward = 1026 #3072 # This is the num of feed forward output from the encoder decoder network\n",
    "dropout = 0.1\n",
    "max_len = embedding_dim\n",
    "trans_activation = \"gelu\"\n",
    "trans_norm_first = False\n",
    "trans_batch_first = True\n",
    "feat_map_dict = btc_usd_train.return_all_output_col_as_dict()\n",
    "del X, Y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "wasiunet_model = WasiuNet(embedding_dim=embedding_dim, inp_feat=inp_feat, out_feat=out_feat, \n",
    "                 in_channels = in_channels, patch_size=patch_size,space_seq_len=space_seq_len,expand_HW=expand_HW,\n",
    "                 nhead=nhead, num_encoder_layers=num_encoder_layers, \n",
    "                 num_decoder_layers=num_decoder_layers, dim_feedforward=dim_feedforward, \n",
    "                 dropout=dropout, max_len=max_len, device=device, trans_activation=trans_activation,\n",
    "                 trans_norm_first=trans_norm_first,trans_batch_first=trans_batch_first,\n",
    "                 feat_map_dict=feat_map_dict\n",
    "                 ).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup logging and checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "tb_logs = \"outputs/tb_logs\"\n",
    "logger = TensorBoardLogger(tb_logs, name='waisiu_net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =None\n",
    "if load_model and path != None:\n",
    "  wasiunet_model = WasiuNet.load_from_checkpoint(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "curr_time = datetime.now().strftime(\"%m-%d-%Y-%H\")\n",
    "model_arch = \"waisiu_net\"\n",
    "train_checkpoint_callback = ModelCheckpoint(dirpath=\"outputs/checkpoint\",every_n_train_steps=1, save_top_k=1,filename=model_arch+\"/train-model-{epoch:02d}-\"+curr_time)\n",
    "val_checkpoint_callback = ModelCheckpoint(dirpath=\"outputs/checkpoint\", mode=\"min\", save_top_k=1, monitor=\"val_loss\",save_on_train_epoch_end=True,filename=model_arch)\n",
    "# model_arch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(limit_train_batches=1000, limit_test_batches=30, limit_val_batches=30, logger=logger,callbacks=[train_checkpoint_callback, val_checkpoint_callback], num_sanity_val_steps=0, default_root_dir=\"outputs/\", accelerator=device_name, devices=-1,enable_model_summary=False,profiler=False, check_val_every_n_epoch=1, fast_dev_run=False, log_every_n_steps=1, max_epochs=num_epochs, overfit_batches=0)\n",
    "trainer.fit(wasiunet_model, train_loader, valid_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9 (main, Dec  7 2022, 13:47:07) [GCC 12.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
